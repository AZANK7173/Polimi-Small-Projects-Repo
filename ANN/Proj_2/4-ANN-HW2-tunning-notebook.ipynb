{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. Environment Set Up and Data Loading**"],"metadata":{"id":"Y7pLxiAhF4LW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVLoJewq36ne"},"outputs":[],"source":["!pip install gdown"]},{"cell_type":"code","source":["!pip install keras-tuner"],"metadata":{"id":"XVXIwAmk39-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gdown\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Bidirectional\n","import random\n","from bayes_opt import BayesianOptimization\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization\n","\n","seed = 72"],"metadata":{"id":"9wEZ7APu4APy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Google Drive URLs for the .npy files\n","url_categ = \"https://drive.google.com/uc?id=1UiZliu_AQdlkjRhVf61Cc0_iJNvIbSyJ\"\n","url_train = \"https://drive.google.com/uc?id=1hIkzsOiDMX5B7pwxyJkiOEBV1nW6_cOB\"\n","url_valid = \"https://drive.google.com/uc?id=1nV6ugTmqf--NTzBZCpb80PO0YZmsjigs\"\n","\n","# Function to download and load a .npy file\n","def download_and_load_npy(url):\n","    output_file = gdown.download(url, quiet=True)\n","    return np.load(output_file)\n","\n","# Downloading and reading the .npy files\n","categories = download_and_load_npy(url_categ)\n","training_dataset = download_and_load_npy(url_train)\n","valid_periods = download_and_load_npy(url_valid)"],"metadata":{"id":"m2camZl74CUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2. Data Preparation**\n","\n","> Converts NumPy arrays into Pandas DataFrames, handles column renaming, and replaces 0 values with NaN."],"metadata":{"id":"qqrAbqrXHhdE"}},{"cell_type":"code","source":["categ_df = pd.DataFrame(categories)#.T\n","train_df = pd.DataFrame(training_dataset)#.T\n","val_df = pd.DataFrame(valid_periods)"],"metadata":{"id":"NyFcGeGb4Esj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf = pd.DataFrame(categories)\n","tdf = pd.DataFrame(training_dataset)\n","vdf = pd.DataFrame(valid_periods)\n","# Rename the columns of our vdf dataframe to Start (the time instance at which the time series starts) and End (time at which it ends)\n","vdf = vdf.rename(columns={0: \"Start\", 1: \"End\"})"],"metadata":{"id":"0oADjxnv4I38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tdf = tdf.replace(0, np.nan)"],"metadata":{"id":"fc55Sm6E4JlG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **3. Handling Missing Values**\n","\n","> Implements a sliding window mean to fill missing values in the time series data.\n","\n","\n"],"metadata":{"id":"QhQaJdm4Hmyk"}},{"cell_type":"code","source":["window_size = 5\n","\n","# Create a copy of the original DataFrame to preserve the original data\n","tdf_filled = tdf.copy()\n","\n","# Iterate through the rows (axis=0) and fill missing values using a sliding window mean\n","for i in range(len(tdf_filled)):\n","    window = tdf_filled.iloc[i].rolling(window=window_size, min_periods=1)\n","\n","    # Calculate the mean within the sliding window\n","    window_mean = window.mean()\n","\n","    # Fill missing values with the calculated mean\n","    tdf_filled.iloc[i] = tdf_filled.iloc[i].fillna(window_mean)\n","\n","tdf_filled"],"metadata":{"id":"-8XPTKu54Let"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tdf = tdf_filled"],"metadata":{"id":"b-36WjmT4NYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **4. Data Scaling**\n","\n","> Applies Robust Scaler and Min-Max Scaler to normalize and scale the time series data.\n","\n"],"metadata":{"id":"leOrbDZfHr-f"}},{"cell_type":"code","source":["def build_scaled_df(df):\n","    # Assuming 'your_df' is your DataFrame\n","    indices = df.index\n","    columns = df.columns\n","\n","    # Apply Robust Scaler\n","    scaler = RobustScaler(with_centering=True)\n","    scaled_array = scaler.fit_transform(df)\n","\n","    min_max_scaler = MinMaxScaler()\n","    scaled_array = min_max_scaler.fit_transform(scaled_array)\n","\n","    # Create a new DataFrame with original indices and columns\n","    scaled_df = pd.DataFrame(data=scaled_array, columns=columns, index=indices)\n","\n","    return scaled_df"],"metadata":{"id":"vsr5KEzf4PBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOTAL = build_scaled_df(tdf)"],"metadata":{"id":"O6knu4Dl4Q02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOTAL = TOTAL.iloc[:,2490:].copy()"],"metadata":{"id":"SyfUX86m4Sg3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **5. Model Bulilding and Training**\n","\n","*   Removes rows with a high percentage of missing values and interpolates remaining missing values.\n","*   Creates input-output sequences for the LSTM model using a sliding window approach.\n","*   Defines a function to build an LSTM model with tunable hyperparameters and performs random search for optimal values.\n","*   Retrieves and displays the summary of the best-performing LSTM model from the hyperparameter tuning.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"dlnBdigSHwNg"}},{"cell_type":"code","source":["def threshold_df(df):\n","    threshold = 0.15\n","    missing_percentage = df.isnull().mean(axis=1)\n","    mask_valid_rows = missing_percentage <= threshold\n","    cleaned_df = df.loc[mask_valid_rows]\n","\n","    return cleaned_df"],"metadata":{"id":"vAj9p2tv4Z5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOTAL = threshold_df(TOTAL)\n","\n","TOTAL.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n","TOTAL.interpolate(method='linear', limit_direction='backward', axis=0, inplace=True)"],"metadata":{"id":"Hf8xmYmp4bgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_sequences(df):\n","    time_steps = 200\n","    future_steps = 9\n","    X, y = [], []\n","    for i in range(len(df)):\n","        for j in range(0, 51, 5):\n","            X.append(df.iloc[i, j:time_steps+j].values)\n","            y.append(df.iloc[i, time_steps+j:time_steps+future_steps+j].values)\n","\n","\n","    # Convert the lists to numpy arrays\n","    X = np.array(X)\n","    y = np.array(y)\n","\n","    # Now y should be of the correct shape, but let's ensure by reshaping if necessary\n","    y = y.reshape(-1, future_steps)\n","\n","    return X, y"],"metadata":{"id":"PXCVuXLi4dFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_total, y_total = build_sequences(TOTAL)"],"metadata":{"id":"nS5MdJa-4e69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_total = np.expand_dims(X_total, axis=2)"],"metadata":{"id":"_PwB4M564gsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_total.shape"],"metadata":{"id":"7G3kyM0S4iWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_total_train, X_total_test, y_total_train, y_total_test = train_test_split(X_total, y_total, test_size=0.2, shuffle=False)"],"metadata":{"id":"tsRrPts84j-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_total_train.shape"],"metadata":{"id":"DnNv76I_4n7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_total_train.shape"],"metadata":{"id":"o4pkoJpr4ppb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from kerastuner.tuners import RandomSearch\n","\n","# Create the LSTM model\n","time_steps = 200\n","future_steps = 9\n","\n","def build_model(hp):\n","    model = Sequential()\n","    # Tune the number of units for the first LSTM layer\n","    hp_units_1 = hp.Int('units_1', min_value=100, max_value=140, step=10)\n","    model.add(LSTM(units=hp_units_1, return_sequences=True, input_shape=(time_steps, 1)))\n","\n","    # Tune the number of units for the second LSTM layer\n","    #hp_units_2 = hp.Int('units_2', min_value=50, max_value=1000, step=50)\n","    model.add(LSTM(units=hp_units_1))\n","\n","    # Dense layer for future steps prediction\n","    model.add(Dense(future_steps))\n","\n","    # Compile the model with a tunable learning rate\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                  loss='mean_squared_error',\n","                  metrics=['MeanSquaredError'])\n","\n","    return model\n","\n","# Define the search space\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_mean_squared_error',\n","    max_trials=10,\n","    executions_per_trial=3,\n","    directory='keras_tuner_logs',\n","    project_name='lstm_hyperparameter_tuning'\n",")\n","\n","# Start the search\n","tuner.search_space_summary()\n","\n","# Assuming you have X_train, y_train, X_val, and y_val defined\n","tuner.search(X_total_train, y_total_train,\n","             validation_data=(X_total_test, y_total_test),\n","             epochs=10,\n","             batch_size=32)\n","\n","# Get the best model\n","best_model = tuner.get_best_models(num_models=1)[0]\n","best_model.summary()"],"metadata":{"id":"GNbZ4V-E4rYH"},"execution_count":null,"outputs":[]}]}