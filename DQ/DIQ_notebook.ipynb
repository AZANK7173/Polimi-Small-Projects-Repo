{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2CBaDF5DxaC"
      },
      "source": [
        "# DIQ Project\n",
        "\n",
        "Project ID: 49\n",
        "\n",
        "Surname: Azank dos Santos\n",
        "\n",
        "Name: Felipe\n",
        "\n",
        "Person Code: 10919711\n",
        "\n",
        "Dataset: ```house.csv```\n",
        "\n",
        "Objective: Comparison between prediction results before and after the Outlier detection and handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bgc_s9ADxaF"
      },
      "source": [
        "## 1 - Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "G1mXjLUWDxaG",
        "outputId": "027e4179-f308-4950-99f2-12b50533a96e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa5fcd158fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# for dataframe manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[0;31m# for array manipulation and math\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdirty_accuracy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minjection\u001b[0m \u001b[0;31m#python code to inject the outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dirty_accuracy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd # for dataframe manipulation\n",
        "import numpy as np  # for array manipulation and math\n",
        "from dirty_accuracy import injection #python code to inject the outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGO-z-XDDxaH"
      },
      "source": [
        "## 2 - Data Importation and common analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "cZPdvBp5DxaI",
        "outputId": "ba518b0a-cfb9-49cd-e274-a303bb3afe7c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9b83291e8d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"house.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'house.csv'"
          ]
        }
      ],
      "source": [
        "df_pandas = pd.read_csv(\"house.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FrrGVERDxaI"
      },
      "outputs": [],
      "source": [
        "df_pandas.info();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-93iVHDxaJ"
      },
      "source": [
        "It's possible to see that the dataset presented, at first, does not show any missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "-CJIuFn9DxaK",
        "outputId": "780ddf6e-2afa-4089-fc70-dcdb99641f60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-811e00c05818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# features overview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_pandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_pandas' is not defined"
          ]
        }
      ],
      "source": [
        "# features overview\n",
        "df_pandas.describe().T;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CftnEXF1DxaK"
      },
      "source": [
        "Using the describe fuction, it's possible to see that the columns of the dataset don't have anything out of the ordinary (values that show a count don't have negative values, extremelly max or min numbers and etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-JW0H_TDxaL"
      },
      "outputs": [],
      "source": [
        "#for column in df_pandas.columns:\n",
        "#   print(f\"{column} unique values: {df_pandas[column].nunique()}, datatype: {df_pandas[column].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApVFsQ_bDxaM"
      },
      "source": [
        "By analysing more closelly, it is possible to see the presence of some columns which, eventhough are presented as integers, should be considered as categorical features when it comes to feature engineering for the Machine Learning part, since there is not a quantity relation with them, this is the case of columns such as \"OverallCond\", and \"YrSold\".\n",
        "\n",
        "However, since the objective is to sudy relations between performance given outliers removal, the dummy_variable generation won't be done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg62jELoDxaM"
      },
      "outputs": [],
      "source": [
        "# example of the above features described\n",
        "df_pandas['OverallCond'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pVH5Jb6DxaN"
      },
      "source": [
        "## 3 - Outlier Imputation\n",
        "\n",
        "Considering that this is a classification problem, the target target value should not be part of the imputation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGxJPUR8DxaN"
      },
      "outputs": [],
      "source": [
        "df_50, df_40, df_30, df_20, df_10 =  injection(\n",
        "    df_pandas,\n",
        "    seed= 72,\n",
        "    name= 'house',\n",
        "    name_class= 'SaleCondition'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH58JYdDDxaO"
      },
      "outputs": [],
      "source": [
        "df_50.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoD-xLVEDxaO"
      },
      "outputs": [],
      "source": [
        "df_pandas.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IliapkvTDxaP"
      },
      "source": [
        "By a quick view in the dataframes, its possible to see that the outlier imputation has succeded and added unusual values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLuvWO28DxaQ"
      },
      "source": [
        "## 4 - Computing Machine Learning Model\n",
        "\n",
        "The two selected machine learning models are: K Nearst Neighbors (KNN) and Support-Vector Machine (SVM).\n",
        "\n",
        "This choice was made once that KNN is know for not handling outliers well, since its a non-parametric model, which leads to problems in extrapolating assumptions for very different data (does not create a function about the data). The SVM, in the other hand, is a parametrical one, which is good for extrapolating assumptions, but not good enough to don't show changes in the model. Studying them may help understand clearly the impact in each type of model.\n",
        "\n",
        "OBS: Tree based Models were not choosen since they usually handle well (specially ensemble models) outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sULChMRuDxaQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier           #KNN\n",
        "from sklearn.svm import SVC                                  #support-vector-classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtnIzEMoDxaQ"
      },
      "source": [
        "## Train_Test Split\n",
        "\n",
        "Before preprocessing of features, it's necessary to split the data in training and test. Since all datasets have the same number of columns, it is possible to make a train split with the same random seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q5EU2BmDxaQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data(df, test_percent=0.3):\n",
        "    # feature columns\n",
        "    X = df.iloc[:,0:-1]\n",
        "\n",
        "    # target column\n",
        "    y = df['SaleCondition']\n",
        "\n",
        "    #train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_percent, random_state=72)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Prd-W3DxaQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, X, y = split_data(df_pandas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkV_UUNDxaR"
      },
      "source": [
        "### Feature Engineeering\n",
        "\n",
        "In order to work with all columns form the dataset, there's need to process the categorical features in valid ones for the ML models. Since this is not the main goal of the project, let's only apply the following transformations:\n",
        "\n",
        "* One_Hot_Encoder for categorical columns: in order to fit it to the models\n",
        "* Standard_Scaler for numerical columns: since KNN is distance sensitive, which means is highly necessary to apply a standarization\n",
        "\n",
        "We apply it by creating an Sci-kit learn pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9vWpuwoDxaR"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline #pipeline construction\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder #preprocessing tecniques\n",
        "from sklearn.compose import ColumnTransformer #in order to split between numerical and categorical features\n",
        "\n",
        "def build_pipelines(X):\n",
        "    # x input is needed for gathering data dimensions\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), list(X.select_dtypes(include=['int','float']).columns)),\n",
        "            ('cat', OneHotEncoder(handle_unknown='infrequent_if_exist'), list(X.select_dtypes(include=['object']).columns))\n",
        "        ])\n",
        "\n",
        "    pipe_1 = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors=5))\n",
        "    pipe_2 = make_pipeline(preprocessor, SVC(kernel='linear', C=1))\n",
        "\n",
        "    return pipe_1, pipe_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrvhoq4pDxaR"
      },
      "outputs": [],
      "source": [
        "pipe_knn, pipe_svm = build_pipelines(X)\n",
        "\n",
        "pipe_knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqB65e8SDxaR"
      },
      "source": [
        "## Fitting and Evaluating\n",
        "\n",
        "In order to evaluate the process, we will use accuracy, precision. The last one should be added to make us able to detect any classification bias in out dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoNpnGZWDxaS"
      },
      "outputs": [],
      "source": [
        "def create_predictons(X_train, y_train, X_test, pipe_1, pipe_2):\n",
        "    pipe_1.fit(X_train, y_train)\n",
        "    pipe_2.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_1 = pipe_1.predict(X_test)\n",
        "    y_pred_2 = pipe_2.predict(X_test)\n",
        "\n",
        "    return y_pred_1, y_pred_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiI8x5rlDxaS"
      },
      "outputs": [],
      "source": [
        "y_pred_knn, y_pred_svm = create_predictons(X_train, y_train, X_test, pipe_knn, pipe_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG7FobyqDxaS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import warnings\n",
        "\n",
        "def evaluate(y_true, y_pred_1, y_pred_2, print_=False):\n",
        "    warnings.filterwarnings('ignore')\n",
        "    #warning needed since its a multi-class classification, which leads to some divisions by 0 when dealing with precision\n",
        "    #this is dealth with by applying the macro strategy to compute (treats these cases as 0), once the\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    acc1 = accuracy_score(y_true, y_pred_1)\n",
        "    prec1 = precision_score(y_true, y_pred_1, average='macro')\n",
        "    scores.append((acc1, prec1))\n",
        "\n",
        "\n",
        "    acc2 = accuracy_score(y_true, y_pred_2)\n",
        "    prec2 = precision_score(y_true, y_pred_2, average='macro')\n",
        "    scores.append((acc2, prec2))\n",
        "\n",
        "    if print_:\n",
        "\n",
        "        print(f\"result pipeline 1 (KNN)\")\n",
        "        print(f\"accuracy: {acc1}\")\n",
        "        print(f\"precision: {prec1}\\n\")\n",
        "\n",
        "        print(f\"result pipeline 2 (SVM)\")\n",
        "        print(f\"accuracy: {acc2}\")\n",
        "        print(f\"precision: {prec2}\")\n",
        "\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hjZ20S3DxaS"
      },
      "outputs": [],
      "source": [
        "scores_df_original = evaluate(y_test, y_pred_knn, y_pred_svm, print_=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL1Q9x0rDxaS"
      },
      "source": [
        "## 5 - Applying for every imputed dataset\n",
        "\n",
        "Now that we have every function and processes well defined, we can apply the pipeline and get the results for all the datasets imputed with outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD14vj5wDxaS"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier           #KNN\n",
        "from sklearn.svm import SVC                                  #support-vector-classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline                      #pipeline construction\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder #preprocessing tecniques\n",
        "from sklearn.compose import ColumnTransformer                   #in order to split between numerical and categorical features\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "import warnings\n",
        "\n",
        "def compute_model(df, test_percent=0.3):\n",
        "\n",
        "    X_train, X_test, y_train, y_test, X, y = split_data(df, test_percent)\n",
        "    pipe_knn, pipe_svm = build_pipelines(X)\n",
        "    y_pred_knn, y_pred_svm = create_predictons(X_train, y_train, X_test, pipe_knn, pipe_svm)\n",
        "    scores_result = evaluate(y_test, y_pred_knn, y_pred_svm)\n",
        "\n",
        "    return scores_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqZghSWIDxaT"
      },
      "outputs": [],
      "source": [
        "compute_model(df_pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIemTBZ_DxaT"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({'Data_Frame':[], 'KNN_accuracy':[], 'KNN_precision_':[], 'SVM_accuracy':[], 'SVM_precision':[]})\n",
        "name = ['original', '10% outliers', '20% outliers', '30% outliers', '40% outliers', '50% outliers' ]\n",
        "i = 0\n",
        "\n",
        "for df in [df_pandas, df_10, df_20, df_30, df_40, df_50]:\n",
        "    score = compute_model(df)\n",
        "\n",
        "    results = results.append({'Data_Frame':name[i], 'KNN_accuracy':score[0][0], 'KNN_precision_':score[0][1],\n",
        "                     'SVM_accuracy':score[1][0], 'SVM_precision':score[1][1]}, ignore_index=True)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN3DkD3kDxaT"
      },
      "source": [
        "Analysing the results, it is possible to see that, aoart from a few unusual cases, all metrics of evaluation of the models decayed. One possible explanation to why did the results didn't show a higher decay must be explained by the high number of categorial features created in the One_Hot_Encoder process, leading to the \"Dimensionality Curse\" problem. By the end of the notebook, its possible to see the impact in performance if we considered only numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Mq4t_jDxaT"
      },
      "source": [
        "## 6 - Outlier detection\n",
        "\n",
        "In order to try increasing the model efficency back to its original process, lets apply two outlier detection algorithms in order to correct the dataset:\n",
        "\n",
        "* Local Outlier Factor (LOF)\n",
        "* Connectivity-Based Outlier Factor (COF) Algorithm\n",
        "\n",
        "### 6.1 - Local Outlier Factor (LOF)\n",
        "\n",
        "Algorithm that stipulates a measurement on how isolated a given point is from its neighbors. This is a good measure because the outliers detected are located outside the min-max values from each column. The application is based in the code developted by the professor during the exercices section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tDegPHeDxaU"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "def LOF_outlier_detection(df,contamination):\n",
        "    clf = LocalOutlierFactor(n_neighbors=4, contamination=contamination) #setting as auto in order to get realistic results\n",
        "\n",
        "    clf.fit_predict(df.select_dtypes(include=['int','float']))\n",
        "\n",
        "    LOF_scores = clf.negative_outlier_factor_\n",
        "\n",
        "    ####print(np.mean(LOF_scores))\n",
        "    ####print(np.std(LOF_scores))\n",
        "\n",
        "    outliers_index = df[LOF_scores < np.mean(LOF_scores - 1*np.std(LOF_scores))].index\n",
        "\n",
        "    return outliers_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoyqnc7WDxaU"
      },
      "source": [
        "### 6.2 - Connectivity-Based Outlier Factor (COF) Algorithm\n",
        "\n",
        "This technique is an improvment of the LOF algorithm, in which a similar measurement of isolation is calculated and then the values are sorted and ranked to detect the outliers in the data. The goal of using this algorithm is to compare if it does a better job than its simpler version. It's implementation can be found in the ```pyod``` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JghQzoVDxaU"
      },
      "outputs": [],
      "source": [
        "from pyod.models.cof import COF\n",
        "\n",
        "def COF_outlier_detection(df, contamination):\n",
        "    X = df.select_dtypes(include=['int','float']).values\n",
        "    clf = COF(n_neighbors=4, contamination=contamination)\n",
        "\n",
        "    clf.fit(X)\n",
        "    outliers = clf.predict(X)\n",
        "\n",
        "    outlier_index = np.where(outliers==1)[0]\n",
        "\n",
        "    return outlier_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcrRiVqZDxaU"
      },
      "source": [
        "## 7 - Outlier Handling\n",
        "\n",
        "Now that we have the defined functions to detect the outliers in the data, it's necessary to decide the methods to substitute the values form the detected problems. Given the target column distribution (mosstly one category) a simple way to deal with the missing value would be by substitute the outlier values by the mean of the feature it belongs (Single Imputation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg1k36zIDxaU"
      },
      "outputs": [],
      "source": [
        "def replace_outliers(df,contamination):\n",
        "    outliers_index_1 = LOF_outlier_detection(df,contamination)\n",
        "    outliers_index_2 = COF_outlier_detection(df,contamination)\n",
        "\n",
        "    df1 = df.copy()\n",
        "    df2 = df.copy()\n",
        "\n",
        "\n",
        "    for column in df.select_dtypes(include=('int',float)):\n",
        "        df1[column].iloc[outliers_index_1] = df[column].drop(outliers_index_1).mean()\n",
        "        df2[column].iloc[outliers_index_2] = df[column].drop(outliers_index_2).mean()\n",
        "\n",
        "    return df1, df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OyVHgZhDxaU"
      },
      "source": [
        "## 8 - Evaluating after Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRmAKUeXDxaU"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({'Data_Frame':[], 'KNN_accuracy':[], 'KNN_precision_':[], 'SVM_accuracy':[], 'SVM_precision':[]})\n",
        "name = [\n",
        "'10% outliers', '10% outliers_LOF_Fixed', '10% outliers_COF_Fixed',\n",
        "'20% outliers', '20% outliers_LOF_Fixed', '20% outliers_COF_Fixed',\n",
        "'30% outliers', '30% outliers_LOF_Fixed', '30% outliers_COF_Fixed',\n",
        "'40% outliers', '40% outliers_LOF_Fixed', '40% outliers_COF_Fixed',\n",
        "'50% outliers', '50% outliers_LOF_Fixed', '50% outliers_COF_Fixed'\n",
        "]\n",
        "contaminations = [0.1,0.1,0.1,0.2,0.2,0.2,0.3,0.3,0.3,0.4,0.4,0.4,0.499,0.499,0.499]\n",
        "contaminations = [0.1]*15\n",
        "i = 0\n",
        "\n",
        "#generate DataFrames\n",
        "for df in [df_10, df_20, df_30, df_40, df_50]:\n",
        "    score = compute_model(df)\n",
        "    results = results.append({'Data_Frame':name[i], 'KNN_accuracy':score[0][0], 'KNN_precision_':score[0][1],\n",
        "                     'SVM_accuracy':score[1][0], 'SVM_precision':score[1][1]}, ignore_index=True)\n",
        "    i+=1\n",
        "\n",
        "    #computing with LOF substitution\n",
        "    df_LOF, df_COF = replace_outliers(df,contamination=contaminations[i])\n",
        "    score = compute_model(df_LOF)\n",
        "    results = results.append({'Data_Frame':name[i], 'KNN_accuracy':score[0][0], 'KNN_precision_':score[0][1],\n",
        "                     'SVM_accuracy':score[1][0], 'SVM_precision':score[1][1]}, ignore_index=True)\n",
        "    i+=1\n",
        "\n",
        "    #computing with COF substitution\n",
        "    score = compute_model(df_COF)\n",
        "    results = results.append({'Data_Frame':name[i], 'KNN_accuracy':score[0][0], 'KNN_precision_':score[0][1],\n",
        "                     'SVM_accuracy':score[1][0], 'SVM_precision':score[1][1]}, ignore_index=True)\n",
        "    i+=1\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l29h43DUDxaV"
      },
      "source": [
        "## 9 - Comparisons and problems with high dimensionality\n",
        "\n",
        "After evaluating the results, it is possible to see that, in general scale, the accuracy of the models increased and the precision in some cases increased drastically. However, there are cases where the opposite situation happens, leading to believe that some oulier values were not substituted properly.\n",
        "\n",
        "One phenomenon that could explain the difficulty to collect the clear outliers is the Dimensionality Curse, that is the process in which, given the high dimensionality of the dataset (multiple columns), the computing of the distance between points begins to grow out of control, becoming extremely hard to select what is a big difference, and what is indeed an outlier.\n",
        "\n",
        "Two ways to dealing with that can be brought to discussion:\n",
        "\n",
        "* 1- Apply Principal Components Analysis (PCA) and try reducing the dimensions: this way the dimensionality curse can become weaker, however the outlier measures will be mixed up with other columns\n",
        "\n",
        "* 2- Apply the outlier detection procedures column-wise: so that we identify the index of each problem given"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV31RyX5DxaV"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "At the end, its possible to see that the outlier handling was indeed helpful in changing the precision metric and had little effect in the accuracy of the model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}